{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4O3Os6OH6GD"
      },
      "source": [
        "## LANGCHAIN WITH PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r4P6Y2L9H6GH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyA7mtxcN9pEbP3xvjgkF1Sda5lOiLlBfIQ\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3-s-6R9H6GJ"
      },
      "source": [
        "### PROMPT TEMPLATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "H-vX9enTH6GJ",
        "outputId": "a2a16d29-a527-428d-b04e-f273a3c8d248"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is a good name for a company that makes colorful socks?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
        "prompt.format(product=\"colorful socks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zolsfocaH6GM"
      },
      "source": [
        "### CHATPROMPT TEMPLATE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr8ggQbgH6GM",
        "outputId": "caae8d42-7173-48df-d449-8ffe84a04ee6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from langchain_core.prompts.chat import ChatPromptTemplate\n",
        "\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "human_template = \"{text}\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", template),\n",
        "    (\"human\", human_template),\n",
        "])\n",
        "\n",
        "chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fM6uEh-XH6GN"
      },
      "source": [
        "### OUTPUT PARSER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzCnZ-RiH6GO",
        "outputId": "23ea75f0-9e15-457c-94c7-0e98577df967"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hi', 'bye']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
        "\n",
        "output_parser = CommaSeparatedListOutputParser()\n",
        "output_parser.parse(\"hi, bye\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zUnsgit7H6GP",
        "outputId": "52348061-2e32-47cc-e131-8b110813b814"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I want to suggest file IT return. \\nIn an easy way, explain the basics of income tax.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "demo_template='''I want to suggest file IT return.\n",
        "In an easy way, explain the basics of {financial_concept}.'''\n",
        "\n",
        "prompt=PromptTemplate(\n",
        "    input_variables=['financial_concept'],\n",
        "    template=demo_template\n",
        "    )\n",
        "\n",
        "prompt.format(financial_concept='income tax')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai\n"
      ],
      "metadata": {
        "id": "d11NkvNMJFnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf3rNwakH6GP",
        "outputId": "0e0de248-87d4-417b-e778-0e217b526620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-316643621.py:11: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain1 = LLMChain(llm=llm, prompt=prompt)\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# Initialize Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # or \"gemini-1.5-pro\"\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# Create an LLMChain with the same prompt\n",
        "chain1 = LLMChain(llm=llm, prompt=prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "uHTX_0VOH6GQ",
        "outputId": "2ec74926-e733-4084-9e80-701c87a53a48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1747960014.py:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  chain1.run('Income Tax')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Imagine the government needs money to build roads, schools, and hospitals.  It gets some money from things like sales taxes, but a big chunk comes from income tax.  Income tax is basically a percentage of the money you earn (your income) that you pay to the government.\\n\\nHere's the simplified breakdown:\\n\\n1. **Income:** This is all the money you earned during the tax year (usually January 1st to December 31st). This includes your salary, wages, business profits, investments, etc.\\n\\n2. **Deductions:**  These are things you can subtract from your income to lower the amount you're taxed on.  Common deductions might include contributions to retirement accounts, certain medical expenses, or mortgage interest (depending on your country's tax laws).\\n\\n3. **Taxable Income:** This is your income *after* you've subtracted your deductions.\\n\\n4. **Tax Rate:**  Governments use a system of tax brackets. This means the percentage you pay in taxes goes up as your income goes up.  For example, you might pay 10% on the first $10,000 you earn, then 15% on the next $20,000, and so on.  This isn't a flat percentage across the board.\\n\\n5. **Tax Liability:** This is the actual amount of tax you owe, calculated based on your taxable income and the tax rates.\\n\\n6. **Filing a Tax Return:**  This is the form you fill out to tell the government your income and calculate your tax liability.  You'll usually do this annually.  If you've paid too much in taxes throughout the year (through things like payroll deductions), you'll get a refund.  If you haven't paid enough, you'll owe more.\\n\\n**In short:** You earn money, the government takes a percentage (your tax), and filing your tax return is how you figure out exactly how much you owe or are owed back.  Each country has its own specific rules and forms, so it's important to understand your country's tax laws.  If you're unsure about anything, it's always best to seek professional advice from a tax accountant or advisor.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "chain1.run('Income Tax')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6xxrRKHH6GR"
      },
      "source": [
        "## LANGUAGE TRANSLATION USING LANGCHAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RXTw3sO9H6GR",
        "outputId": "b5b051d7-d5f3-4c63-ba64-cad018c1f56d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In an easy way translate the following sentence 'Generativa AI & LLM is the future for next 3 years' into hindi\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template='''In an easy way translate the following sentence '{sentence}' into {target_language}'''\n",
        "language_prompt = PromptTemplate(\n",
        "    input_variables=[\"sentence\",'target_language'],\n",
        "    template=template,\n",
        ")\n",
        "language_prompt.format(sentence=\"Generativa AI & LLM is the future for next 3 years\",target_language='hindi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_EhYxc-H6GS",
        "outputId": "5d1cfc2f-36fb-4eca-ad84-fa2d60299057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1278528551.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  chain2({'sentence':\"Generativa AI & LLM is the future for next 3 years\",'target_language':'hindi'})\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': 'Generativa AI & LLM is the future for next 3 years',\n",
              " 'target_language': 'hindi',\n",
              " 'text': 'The simplest translation would be:\\n\\n**जनरेटिव AI और LLM अगले 3 सालों का भविष्य हैं।**  (Janeretiv AI aur LLM agale 3 saalon ka bhavishya hain.)\\n\\n\\nA slightly more natural-sounding option, emphasizing the *future* aspect:\\n\\n**आगामी 3 वर्षों में जनरेटिव AI और LLM का भविष्य है।** (Aagami 3 varshon mein janeretiv AI aur LLM ka bhavishya hai.)\\n\\n\\nBoth translations convey the same meaning.  The second one might be preferred in formal writing.'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "chain2=LLMChain(llm=llm,prompt=language_prompt)\n",
        "\n",
        "chain2({'sentence':\"Generativa AI & LLM is the future for next 3 years\",'target_language':'hindi'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmC7KQHAH6GS"
      },
      "source": [
        "### langchain fewshotprompttemplate\n",
        "https://python.langchain.com/docs/modules/model_io/prompts/few_shot_examples_chat/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "k1mozz7xH6GT"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqW3o3AKH6GT",
        "outputId": "d3ad39bd-d6ce-45ba-90f4-c0746c36c2ad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'input': '2+2', 'output': '4'}, {'input': '2+3', 'output': '5'}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "examples = [\n",
        "    {\"input\": \"2+2\", \"output\": \"4\"},\n",
        "    {\"input\": \"2+3\", \"output\": \"5\"},\n",
        "]\n",
        "examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB6xE34-H6GT",
        "outputId": "df5f3ba8-48aa-4220-e85d-94534a1ce81a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: 2+2\n",
            "AI: 4\n",
            "Human: 2+3\n",
            "AI: 5\n"
          ]
        }
      ],
      "source": [
        "# This is a prompt template used to format each individual example.\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "print(few_shot_prompt.format())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "I6klt_wHH6GU"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, FewShotPromptTemplate\n",
        "\n",
        "# First, create the list of few shot examples.\n",
        "examples = [\n",
        "    {\"word\": \"success\", \"antonym\": \"failure\"},\n",
        "    {\"word\": \"placed\", \"antonym\": \"not placed\"},\n",
        "]\n",
        "\n",
        "# Next, we specify the template to format the examples we have provided.\n",
        "# We use the `PromptTemplate` class for this.\n",
        "example_formatter_template = \"\"\"Word: {word}\n",
        "Antonym: {antonym}\n",
        "\"\"\"\n",
        "\n",
        "example_prompt = PromptTemplate(\n",
        "    input_variables=[\"word\", \"antonym\"],\n",
        "    template=example_formatter_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kQA7NY9dH6GV"
      },
      "outputs": [],
      "source": [
        "# Finally, we create the `FewShotPromptTemplate` object.\n",
        "few_shot_prompt = FewShotPromptTemplate(\n",
        "    # These are the examples we want to insert into the prompt.\n",
        "    examples=examples,\n",
        "    # This is how we want to format the examples when we insert them into the prompt.\n",
        "    example_prompt=example_prompt,\n",
        "    # The prefix is some text that goes before the examples in the prompt.\n",
        "    # Usually, this consists of intructions.\n",
        "    prefix=\"Give the antonym of every input\\n\",\n",
        "    # The suffix is some text that goes after the examples in the prompt.\n",
        "    # Usually, this is where the user input will go\n",
        "    suffix=\"Word: {input}\\nAntonym: \",\n",
        "    # The input variables are the variables that the overall prompt expects.\n",
        "    input_variables=[\"input\"],\n",
        "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
        "    example_separator=\"\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiJ92FtZH6GV",
        "outputId": "efe18b79-d03b-452a-ca7a-efe2c81fb195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Give the antonym of every input\n",
            "\n",
            "Word: success\n",
            "Antonym: failure\n",
            "\n",
            "Word: placed\n",
            "Antonym: not placed\n",
            "\n",
            "Word: big\n",
            "Antonym: \n"
          ]
        }
      ],
      "source": [
        "#print(few_shot_prompt.format)\n",
        "print(few_shot_prompt.format(input='big'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73T0xbTdH6GW",
        "outputId": "da01334d-a8e9-4de3-b5ee-afa629357642"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'big', 'text': 'small'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "chain=LLMChain(llm=llm,prompt=few_shot_prompt)\n",
        "chain({'input':\"big\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xcw9n4ZGH6GW"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}