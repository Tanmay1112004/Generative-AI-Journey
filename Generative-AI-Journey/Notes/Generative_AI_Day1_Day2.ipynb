{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb47bd25",
   "metadata": {},
   "source": [
    "# Generative AI Notes Handbook\n",
    "---\n",
    "This notebook contains structured notes from **Day-1 & Day-2** of Generative AI class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a51dd",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Day 1 â€“ Introduction to Generative AI\n",
    "\n",
    "## 1. What is Generative AI?\n",
    "- Branch of AI focused on **creating new content** (text, image, audio, video, code).\n",
    "- Works by learning patterns from data and generating outputs.\n",
    "\n",
    "Examples:\n",
    "- ChatGPT (text)\n",
    "- DALLÂ·E, MidJourney (images)\n",
    "- Synthesia (video)\n",
    "- Jukebox (music)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Types of AI\n",
    "- **Narrow AI:** Specialized task (e.g., Alexa, Siri)\n",
    "- **General AI:** Human-like intelligence (still research stage)\n",
    "- **Super AI:** Beyond human intelligence (theory)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Machine Learning Basics\n",
    "- **Supervised Learning:** Uses labeled data (input â†’ output).\n",
    "- **Unsupervised Learning:** Finds patterns without labels.\n",
    "- **Reinforcement Learning:** Learns by trial & error.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Deep Learning\n",
    "- Subset of ML using **Neural Networks** (ANN, CNN, RNN, Transformers).\n",
    "- Core for **Generative AI**.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Difference: Traditional AI vs Generative AI\n",
    "| Traditional AI | Generative AI |\n",
    "|----------------|---------------|\n",
    "| Rule-based | Pattern-learning |\n",
    "| Predicts outputs | Creates new data |\n",
    "| Example: Spam filter | Example: ChatGPT |\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Applications of Generative AI\n",
    "- Text: ChatGPT, Bard, Claude\n",
    "- Image: MidJourney, Stable Diffusion\n",
    "- Video: Synthesia\n",
    "- Audio: Jukebox, ElevenLabs\n",
    "- Code: GitHub Copilot\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Why Generative AI is Important\n",
    "- Enhances productivity\n",
    "- Saves cost & time\n",
    "- Automates creativity\n",
    "- Enables personalized experiences\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2af5ee",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Day 2 â€“ Large Language Models (LLMs)\n",
    "\n",
    "## 1. What is a Large Language Model (LLM)?\n",
    "- AI model trained on massive text datasets.\n",
    "- Famous: GPT-3.5, GPT-4.\n",
    "- \"Large\" because of billions of tokens + parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. How LLMs Work\n",
    "### ðŸ”¹ Tokenization\n",
    "- Text â†’ Tokens\n",
    "- Methods: BPE, WordPiece, SentencePiece\n",
    "\n",
    "### ðŸ”¹ Embeddings\n",
    "- Tokens â†’ vectors\n",
    "- Used in vector DB + RAG\n",
    "\n",
    "### ðŸ”¹ Transformers\n",
    "- Based on **Attention mechanism**\n",
    "- Encoder-only (BERT), Decoder-only (GPT), Encoder-decoder (T5)\n",
    "\n",
    "### ðŸ”¹ Training Process\n",
    "- Pre-training: General data (very costly)\n",
    "- Fine-tuning: Task-specific (cheaper)\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Key Concepts\n",
    "- Prompt Design\n",
    "- Few-shot & Zero-shot learning\n",
    "- Transfer Learning\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Challenges (Without API)\n",
    "- Huge infra cost\n",
    "- Slow inference\n",
    "- Limited POC\n",
    "- Better to use APIs (OpenAI, Google, Meta, Anthropic)\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Ecosystem\n",
    "- LangChain, Pinecone, FAISS, Weaviate\n",
    "- LLaMA 2, Claude, GPT-4, Gemini\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Applications\n",
    "- Chatbots\n",
    "- Summarization\n",
    "- Code gen (Copilot)\n",
    "- Education, Legal, Healthcare\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸŽ¯ Interview Prep â€“ Q&A\n",
    "\n",
    "**Q1. What is LLM?** â†’ AI model trained on massive datasets that generates human-like text.  \n",
    "**Q2. Why Large?** â†’ Parameters + tokens are huge.  \n",
    "**Q3. Tokenization?** â†’ Splits text into tokens (BPE, WordPiece, SentencePiece).  \n",
    "**Q4. Embeddings?** â†’ Vector representation of text.  \n",
    "**Q5. Attention?** â†’ Focuses on important words in context.  \n",
    "**Q6. Pre-training vs Fine-tuning?** â†’ Pre-training = general, Fine-tuning = specific.  \n",
    "**Q7. Prompt Engineering?** â†’ Designing inputs to get best output.  \n",
    "**Q8. Few-shot vs Zero-shot?** â†’ Few-shot = examples, Zero-shot = none.  \n",
    "**Q9. Why use APIs?** â†’ Training cost is too high.  \n",
    "**Q10. Real-world example?** â†’ Customer chatbot, GitHub Copilot.  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
