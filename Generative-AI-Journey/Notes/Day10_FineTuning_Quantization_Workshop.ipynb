{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21483ec8",
   "metadata": {},
   "source": [
    "\n",
    "# üöÄ Day 10 ‚Äì Fine-tuning, Quantization & Performance Optimization\n",
    "\n",
    "This notebook covers **advanced concepts in LLM optimization** including **Fine-tuning, PEFT (LoRA & QLoRA), Quantization, and Performance Optimization techniques**.  \n",
    "These notes are designed for **easy revision** and **interview preparation**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Fine-tuning in LLMs\n",
    "\n",
    "**What is Fine-tuning?**  \n",
    "Fine-tuning means adapting a **pre-trained LLM** (like LLaMA, GPT, Gemini) to a **specific domain/task** by training it further on domain-specific data.\n",
    "\n",
    "### üîë Key Points:\n",
    "- Full parameter fine-tuning = all model parameters updated (very expensive, GPU heavy).\n",
    "- Allows models to adapt to **new data** not present during pre-training.\n",
    "- Common in **domain-specific LLM applications** (e.g., medical chatbots, financial assistants).\n",
    "\n",
    "### Example (Hugging Face - Pseudocode)\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned-model\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=my_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìå PEFT (Parameter Efficient Fine-Tuning)\n",
    "\n",
    "Instead of tuning **all parameters**, PEFT updates **small subsets** of parameters ‚Üí faster & cheaper.\n",
    "\n",
    "### üîë Techniques:\n",
    "1. **LoRA (Low Rank Adaptation)**\n",
    "   - Inserts trainable matrices into transformer layers.\n",
    "   - Only small % of weights updated (saves compute).\n",
    "\n",
    "2. **QLoRA (Quantized LoRA)**\n",
    "   - Like LoRA, but uses **quantized weights (4-bit/8-bit)** ‚Üí saves **GPU memory**.\n",
    "   - Example: Fine-tuning LLaMA 13B with a single 24GB GPU.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Quantization\n",
    "\n",
    "**Definition:** Converting high precision weights (FP32) into lower precision (FP16, INT8, INT4).  \n",
    "This reduces **memory usage** and **inference time**.\n",
    "\n",
    "### üîë Quantization Types:\n",
    "- **Weight-only** ‚Üí Only weights quantized (activations stay full precision).\n",
    "- **Dynamic** ‚Üí Weights pre-quantized, activations quantized on-the-fly.\n",
    "- **Static** ‚Üí Both weights + activations quantized after calibration.\n",
    "- **QAT (Quantization Aware Training)** ‚Üí Simulates quantization during training.\n",
    "\n",
    "### Example (Hugging Face Quantization)\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "### Trade-offs:\n",
    "- ‚úÖ Smaller model size, faster inference.\n",
    "- ‚ùå Possible drop in accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Performance Optimization in LLMs\n",
    "\n",
    "1. **Batching**  \n",
    "   - Process multiple requests together ‚Üí increases throughput.  \n",
    "   - Trade-off: Higher latency for individual requests.\n",
    "\n",
    "2. **KV Caching**  \n",
    "   - Store Key & Value projections during inference.  \n",
    "   - Speeds up token generation massively.  \n",
    "   - Uses more memory.\n",
    "\n",
    "3. **Fused Kernels (e.g., FlashAttention)**  \n",
    "   - Combine multiple GPU operations into one.  \n",
    "   - Reduces memory I/O, boosts speed.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Interview Style Q&A\n",
    "\n",
    "### Q1. What is Fine-tuning?  \n",
    "üëâ Fine-tuning means adapting a pre-trained LLM on domain/task-specific data. It updates model parameters to specialize in a new context.\n",
    "\n",
    "---\n",
    "\n",
    "### Q2. What are PEFT techniques?  \n",
    "üëâ Methods like LoRA & QLoRA which reduce GPU cost by updating only a small subset of weights.\n",
    "\n",
    "---\n",
    "\n",
    "### Q3. Difference between LoRA vs QLoRA?  \n",
    "- **LoRA** ‚Üí Low Rank Adaptation (uses FP16/FP32 weights).  \n",
    "- **QLoRA** ‚Üí LoRA + Quantization (4-bit/8-bit weights), much more memory-efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### Q4. Difference between Fine-tuning vs Quantization?  \n",
    "- **Fine-tuning** ‚Üí Updating parameters to adapt model for new tasks.  \n",
    "- **Quantization** ‚Üí Compressing weights to make inference faster & memory-efficient.\n",
    "\n",
    "---\n",
    "\n",
    "### Q5. Difference between RAG vs LLM?  \n",
    "- **LLM** ‚Üí Generates answers from pre-trained knowledge only.  \n",
    "- **RAG** ‚Üí Retrieves fresh external data + LLM generates response.\n",
    "\n",
    "---\n",
    "\n",
    "### Q6. Difference between Gen AI vs Agentic AI?  \n",
    "- **Gen AI** ‚Üí Creates new content (text, images, audio).  \n",
    "- **Agentic AI** ‚Üí Goal-oriented, uses planning, tools, memory, reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### Q7. Difference between ReLU vs GELU?  \n",
    "- **ReLU** ‚Üí `max(0,x)`, faster but simple.  \n",
    "- **GELU** ‚Üí Smooth curve, better for transformers, handles negative values gracefully.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Resume Keywords\n",
    "- Fine-tuning with **LoRA & QLoRA** on LLaMA models.  \n",
    "- Quantization using **BitsAndBytes (4-bit, 8-bit)**.  \n",
    "- Performance optimized inference using **KV caching & FlashAttention**.  \n",
    "- RAG pipelines using **LangChain & LlamaIndex**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Practical Notes\n",
    "- Always **restart Google Colab runtime** after running large models.  \n",
    "- Use **Hugging Face + Groq** for deployment of large LLMs.  \n",
    "- For interviews: focus on **theory differences + practical trade-offs**.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
