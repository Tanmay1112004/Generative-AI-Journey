{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1508cf59",
   "metadata": {},
   "source": [
    "# üìò Generative AI Notes ‚Äì Day 4 & Day 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebc8f02",
   "metadata": {},
   "source": [
    "\n",
    "## üîë OpenAI API Key\n",
    "- Needed for Generative AI projects  \n",
    "- Website: [platform.openai.com](https://platform.openai.com)  \n",
    "- Access your personal **API Key** here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b89a3",
   "metadata": {},
   "source": [
    "\n",
    "## ‚öôÔ∏è LLM Parameters (OpenAI)\n",
    "1. **Temperature** ‚Äì Controls creativity.  \n",
    "2. **Top P (Nucleus Sampling)** ‚Äì Probability cutoff for tokens.  \n",
    "3. **Other Params**:  \n",
    "   - `text.format`, `effort`, `verbosity`, `tool-choice`, `store`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d440c",
   "metadata": {},
   "source": [
    "\n",
    "## üß™ Day-4 Projects & Learnings\n",
    "\n",
    "### LangChain PoCs\n",
    "- **PoC1:** LangChain + Gemini  \n",
    "- **PoC2:** Language conversion  \n",
    "- **PoC3:** Agents with Pandas dataframe  \n",
    "- **PoC4:** Prompt templates  \n",
    "- **PoC5:** LLMOps with LangSmith  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f453d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: LangChain Agent with Pandas\n",
    "from langchain_experimental.agents import create_pandas_dataframe_agent\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample dataframe\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [25, 30, 35]\n",
    "})\n",
    "\n",
    "# Initialize LLM\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Create Pandas Agent\n",
    "agent = create_pandas_dataframe_agent(llm, df, verbose=True)\n",
    "\n",
    "# Example Query\n",
    "agent.run(\"What is the average age?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec17d7f",
   "metadata": {},
   "source": [
    "\n",
    "### Key Concepts\n",
    "- **Observability** ‚Üí Ability to understand system behavior.  \n",
    "- **Monitoring** ‚Üí Tracking model performance in real-time.  \n",
    "\n",
    "**Project:** Custom GPT model using Gemini API key, monitored with **LangSmith**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad1094",
   "metadata": {},
   "source": [
    "\n",
    "## üß™ Day-5 Projects & Learnings\n",
    "\n",
    "### Recap\n",
    "- LLM + LangChain basics  \n",
    "- LangChain docs (Core + Community)  \n",
    "- Agents writing Python code  \n",
    "- Prompt elements  \n",
    "- Custom GPT + LangSmith monitoring  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b0aec9",
   "metadata": {},
   "source": [
    "\n",
    "### üî• LangGraph\n",
    "Workflow library on top of LangChain.\n",
    "\n",
    "- **Nodes** = LLM calls / function calls  \n",
    "- **Edges** = transitions between steps  \n",
    "- **State** = shared memory  \n",
    "- **END** = stop execution  \n",
    "\n",
    "LangChain ‚Üí DAGs (Directed Acyclic Graphs)  \n",
    "LangGraph ‚Üí Graphs (with cycles, more complex workflows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab67e4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: LangGraph Skeleton\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "# Define graph\n",
    "graph = StateGraph()\n",
    "\n",
    "# Example Node (LLM Call)\n",
    "def chatbot_node(state):\n",
    "    user_input = state.get(\"input\")\n",
    "    response = f\"Chatbot response to: {user_input}\"\n",
    "    return {\"response\": response}\n",
    "\n",
    "# Add node to graph\n",
    "graph.add_node(\"chatbot\", chatbot_node)\n",
    "\n",
    "# Define transitions\n",
    "graph.add_edge(\"start\", \"chatbot\")\n",
    "graph.add_edge(\"chatbot\", \"end\")\n",
    "\n",
    "# Compile and run\n",
    "app = graph.compile()\n",
    "result = app.invoke({\"input\": \"Hello, how are you?\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a6c6e",
   "metadata": {},
   "source": [
    "\n",
    "## üé§ Interview Q&A\n",
    "\n",
    "**Q1. Difference between Temperature and Top-P?**  \n",
    "- Temperature ‚Üí randomness  \n",
    "- Top-P ‚Üí probability cutoff  \n",
    "\n",
    "**Q2. What is LLMOps?**  \n",
    "- Tools + practices (LangSmith) for monitoring, debugging, deploying LLMs.  \n",
    "\n",
    "**Q3. What are LangChain Agents?**  \n",
    "- Agents decide tools, write code, and execute developer-like tasks.  \n",
    "\n",
    "**Q4. LangChain vs LangGraph?**  \n",
    "- LangChain ‚Üí framework, connectors  \n",
    "- LangGraph ‚Üí workflow orchestration with state + cycles  \n",
    "\n",
    "**Q5. Observability vs Monitoring?**  \n",
    "- Observability ‚Üí logs, metrics, traces to understand system  \n",
    "- Monitoring ‚Üí track performance metrics continuously  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
