{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "02c9fe28", "cell_type": "markdown", "source": "\n# Day 7 & 8 \u2013 Generative AI Workshop Notes\n\n---\n\n## Day 7 \u2013 Stable Diffusion + Hugging Face Projects\n\n### 1. What is Stable Diffusion?\n\n- **Stable Diffusion** is a **deep learning text-to-image model** created by **Stability AI**.  \n- It generates high-quality, realistic images from a text prompt.  \n- It\u2019s **open-source** and available on **Hugging Face** (model hub).  \n- Works using **diffusion models**:  \n  - Start with random noise \u2192 step by step remove noise \u2192 generate final image.  \n\n\u2705 **Think of it as:** \"I tell the model a sentence \u2192 it paints a picture for me.\"\n\n---\n\n### 2. Parameters in LLMs\n\n**Parameters** = The \u201cneurons\u201d or **weights** of the model.  \n- They control how the model understands and generates text/images.  \n- More parameters = higher capacity (but also higher cost).  \n\n**Examples:**  \n- GPT-3 \u2192 175 Billion parameters.  \n- Stable Diffusion \u2192 ~890M parameters.  \n- Gemma (Google) \u2192 ranges from 2B to 27B.  \n\n\u2705 **Interview Tip:** If asked *\u201cWhat are parameters in LLMs?\u201d* \u2192  \nSay: *Parameters are the adjustable weights that determine how the model processes input and generates output. They\u2019re like the brain cells of AI.*  \n\n---\n\n### 3. Hugging Face Projects (Using Stability AI Models)\n\n1. **Text Generation**  \n   - Model: `stabilityai/stable-code-instruct-3b`  \n   - Use: Writing code / generating structured text.  \n\n2. **Text-to-Image**  \n   - Model: `stabilityai/stable-diffusion-xl-base-1.0`  \n   - Use: Convert text \u2192 high-quality images.  \n\n3. **Text-to-Audio**  \n   - Model: `stabilityai/stable-audio-open-1.0`  \n   - Use: Generate sound/music/audio files from text prompts.  \n\n4. **Image-to-Image**  \n   - Use: Modify one image into another (e.g., style transfer, editing).  \n\n5. **Image-to-Text**  \n   - Use: Convert images into captions (like describing an image).  \n\n6. **Image-to-Video**  \n   - Use: Extend static images into videos.  \n\n\u2705 **These are end-to-end Generative AI projects you can mention in resume.**\n\n---\n\n### 4. Interview Q&A (Stable Diffusion + Hugging Face)\n\n**Q1:** What is Stable Diffusion?  \n**A1:** It\u2019s a deep learning model that generates images from text prompts using diffusion techniques. Developed by Stability AI, available on Hugging Face.  \n\n**Q2:** What are parameters in LLMs?  \n**A2:** Parameters are the trainable weights of the model that determine how it processes and generates data.  \n\n**Q3:** Name some projects using Stability AI models.  \n**A3:** Text generation (stable-code), text-to-image (SDXL), text-to-audio, image-to-image, image-to-text, image-to-video.  \n\n**Q4:** Why Hugging Face is popular?  \n**A4:** Because it provides pre-trained models, easy APIs, community contributions, and supports multiple modalities (text, image, audio).  \n\n---\n\n## Day 8 \u2013 Google Models\n\n### 1. Google DeepMind\n\n- Google\u2019s **research company** for cutting-edge AI.  \n- Focuses on **large language models, multimodal AI, and applied research**.  \n\n---\n\n### 2. Gemini Models (Google\u2019s Flagship LLM)\n\n- **Gemini 2.5 Pro** \u2192 High performance, reasoning-heavy tasks.  \n- **Gemini 2.5 Flash** \u2192 Faster, lightweight, cheaper to run.  \n- **Gemini 2.5 Flash Image** \u2192 Specially designed for image tasks.  \n- **Gemini 2.5 Flash Pro** \u2192 Combo: fast + powerful + multimodal.  \n\n\u2705 **Gemini = Google\u2019s answer to OpenAI\u2019s GPT-4.**\n\n---\n\n### 3. Gemma Models\n\n- **Gemma 3** \u2192 Base LLM.  \n- **Gemma 3n** \u2192 Optimized for efficiency.  \n- **Shield Gemma** \u2192 Safer AI model with content filtering.  \n\n\u2705 **Gemma = lightweight, open-source alternative to Gemini.**\n\n---\n\n### 4. Google Generative Models (Other)\n\n- **Image Gen** \u2013 Google\u2019s image generation model.  \n- **Lyria** \u2013 AI music generation.  \n- **Veo** \u2013 AI video generation model.  \n\n---\n\n### 5. State-of-the-Art (SOTA) Multimodals\n\n- SOTA = **State of the Art**, meaning **best in class performance**.  \n- Google\u2019s **Gemini models** are multimodal \u2192 handle text + images + audio + video.  \n\n\u2705 **Example:** Ask Gemini: *\u201cSummarize this video and also generate a chart from it.\u201d*  \n\u2192 It can understand video, generate text + visuals.  \n\n---\n\n### 6. Interview Q&A (Google Models)\n\n**Q1:** Difference between Gemini and Gemma?  \n**A1:** Gemini = Google\u2019s flagship multimodal model (large, closed-source). Gemma = lightweight, open-source model family.  \n\n**Q2:** What is SOTA in AI?  \n**A2:** SOTA = State of the Art, meaning the most advanced models with highest benchmark performance.  \n\n**Q3:** What is Gemini 2.5 Flash?  \n**A3:** It\u2019s a fast, efficient version of Gemini designed for real-time applications.  \n\n**Q4:** Name Google\u2019s generative models.  \n**A4:** Gemini, Gemma, Image Gen, Lyria (music), Veo (video).  \n\n---\n\n## 7. Key Revision Points (Day 7 & 8)\n\n- **Stable Diffusion** = Text-to-image model by Stability AI.  \n- **Parameters** = Trainable weights in LLMs.  \n- **Hugging Face** = Hub for models (text, image, audio, video).  \n- **Google Models**:  \n  - **Gemini** \u2192 Flagship, multimodal, SOTA.  \n  - **Gemma** \u2192 Lightweight, open-source.  \n  - **Other** \u2192 Lyria (music), Veo (video).  \n\n\u2705 Always relate models to **projects and real-world applications** in interviews.\n", "metadata": {}}]}