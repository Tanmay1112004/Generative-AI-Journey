{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "0d2c6fa9", "cell_type": "markdown", "source": "\n# Day 9 \u2013 Meta LLaMA Models, Fine-tuning & Quantization\n\n---\n\n## 1. Meta\u2019s LLaMA Models\n\n- **Meta AI** (formerly Facebook AI Research) has released the **LLaMA (Large Language Model Meta AI)** family.  \n- LLaMA models are **open-source** (unlike OpenAI GPT & Google Gemini, which are closed).  \n- Developers can:  \n  - Download and experiment.  \n  - Fine-tune for tasks.  \n  - Run locally (with right hardware).  \n\n### \ud83d\udd39 Versions of LLaMA\n- **LLaMA 3 \u2192** 3.1, 3.2, 3.3  \n- **LLaMA 4 \u2192** Scout, Maverick, Behemoth (Preview)  \n\nThese models have **billions of parameters**, so they require **Hugging Face, Groq, or quantization** to run efficiently.\n\n---\n\n## 2. Why LLaMA over OpenAI & Gemini?\n\n- **OpenAI (ChatGPT / GPT-4)** \u2192 \u274c Fine-tuning limited.  \n- **Google Gemini** \u2192 \u2705 Fine-tuning possible, but limited accuracy.  \n- **Meta LLaMA** \u2192 \u2705 Fully open-source, \u2705 Fine-tuning supported, \u2705 Works with LangChain & LlamaIndex.  \n\n---\n\n## 3. Fine-tuning in LLMs\n\n**Definition:** Adapting a pre-trained LLM for a specific dataset/task.\n\n### Types:\n- **Full Parameter Fine-tuning** \u2192 All layers updated, best accuracy, but expensive.  \n- **PEFT (Parameter Efficient Fine-Tuning)** \u2192 Only small subset updated.  \n  - **LoRA (Low Rank Adaptation)**  \n  - **QLoRA (Quantized LoRA)**  \n\n\ud83d\udc49 Example: Fine-tune LLaMA-2 13B on customer support chats.  \n\n**Resume Line:** *Implemented fine-tuning using LoRA & QLoRA on Meta\u2019s LLaMA models for domain-specific tasks.*\n\n---\n\n## 4. Quantization\n\n**Definition:** Reducing model precision (weights) from FP16/32 \u2192 INT8/INT4.  \n\n- Goal \u2192 Run heavy models on **smaller GPUs (24GB VRAM) or CPU**.  \n- Example:  \n  - LLaMA 13B FP16 \u2192 80GB GPU.  \n  - LLaMA 13B QLoRA 4-bit \u2192 Single 24GB GPU.  \n\n---\n\n## 5. Tools with LLaMA\n\n- **LangChain** \u2192 Build LLM-powered apps (chatbots, RAG).  \n- **LlamaIndex** \u2192 Connect LLMs with external data sources.  \n\n---\n\n## 6. Fine-tuning vs RAG\n\n| Aspect | Fine-tuning | RAG |\n|--------|-------------|-----|\n| **Definition** | Update model weights | Keep weights fixed, add external knowledge |\n| **Cost** | High | Low |\n| **Use Case** | Domain-specific learning | Latest, dynamic info |\n| **Example** | Medical chatbot trained on medical books | Chatbot retrieving from hospital docs |\n\n\ud83d\udc49 Interview Tip: *Fine-tuning = permanent knowledge, RAG = temporary injection.*\n\n---\n\n## 7. Hyperparameter Tuning in ML\n\n- **GridSearchCV** \u2192 Exhaustive search.  \n- **RandomSearchCV** \u2192 Random combinations, faster.  \n\n---\n\n## 8. Hugging Face LLaMA Models\n\n- **LLaMA 2**  \n- **LLaMA 3**  \n- **LLaMA 4 (Preview)**  \n\n(Gated access via Meta + Hugging Face).  \n\n---\n\n## 9. Resume Highlights\n\n- Fine-tuning with **LoRA & QLoRA**.  \n- Quantization for efficient inference.  \n- LangChain + LlamaIndex integration.  \n- Deployment via Hugging Face + Groq.  \n\n---\n\n## 10. Interview Q&A\n\n**Q1. What is fine-tuning in LLMs?**  \nA1. Fine-tuning means retraining a pre-trained model on domain data. It can be full (all parameters) or efficient (LoRA, QLoRA).  \n\n**Q2. Difference between LoRA and QLoRA?**  \nA2. LoRA adds low-rank adapters. QLoRA combines LoRA with quantization (4-bit/8-bit), making it more resource-friendly.  \n\n**Q3. What is quantization?**  \nA3. Technique to reduce precision of model weights to run models on smaller GPUs/CPUs.  \n\n**Q4. Fine-tuning vs RAG?**  \nA4. Fine-tuning permanently updates model knowledge. RAG augments model prompts with retrieved external data.  \n\n**Q5. Why LLaMA is preferred for fine-tuning?**  \nA5. It is open-source, supports PEFT methods, and widely integrated with LangChain/LlamaIndex.  \n\n---\n\n\u2705 Day 9 mastery: Meta LLaMA, Fine-tuning (LoRA/QLoRA), Quantization, RAG comparison.\n", "metadata": {}}]}