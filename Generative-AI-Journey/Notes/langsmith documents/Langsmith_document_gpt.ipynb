{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CMDK6noLb5S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What is LangSmith (in simple terms)\n",
        "\n",
        "* LangSmith is a tool (platform + SDK + UI) made by the LangChain team to help you **monitor, debug, evaluate, and manage** your applications built using LLMs (large language models). ([LangSmith Docs][1])\n",
        "* Why you need it: LLM-based apps are ‚Äúnon-deterministic‚Äù ‚Äî same input can give different outputs, things go wrong, unpredictable behavior. So just logging errors isn‚Äôt enough. You need **traceability**, **metrics**, **evaluation**, and tools to iterate prompts. LangSmith gives you that. ([LangChain][2])\n",
        "* It is **framework-agnostic**. That means you don‚Äôt necessarily need to use LangChain (though it integrates super nicely). Even if your app isn‚Äôt built with LangChain, you can still send traces / logs / evaluations via standard protocols. ([LangChain][2])\n",
        "* Key pillars:\n",
        "\n",
        "  1. **Observability / Tracing** ‚Äî see step-by-step internal execution (which prompts, tool calls, responses)\n",
        "  2. **Evals / Evaluation** ‚Äî test and score your app‚Äôs outputs (compare with reference, use humans or LLMs as judges)\n",
        "  3. **Prompt Engineering / Versioning / Collaboration** ‚Äî build prompt playgrounds, version control, let team iterate\n",
        "  4. **Dashboards / Metrics / Alerts** ‚Äî track cost, latency, error rates, quality metrics over time ([LangSmith Docs][1])\n",
        "\n",
        "So LangSmith = the ‚Äúobservability + testing + debugging layer‚Äù for LLM apps.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Core components / features in detail\n",
        "\n",
        "Let‚Äôs dive into what LangSmith gives you, component-wise.\n",
        "\n",
        "### 2.1 Observability / Tracing\n",
        "\n",
        "* **Trace** = recording what happens inside your LLM application: e.g. what prompt was sent, what the LLM response was, any tool calls, intermediate steps (‚Äúchild runs‚Äù), metadata, tags, etc.\n",
        "* With tracing, you can **visualize** the sequence or tree of steps your application took for a request ‚Äî helps debug weird outputs or unexpected branches. ([LangChain Docs][3])\n",
        "* You can connect LangSmith tracing with LangChain (Python / JS) easily. E.g. using environment variables or `traceable` contexts. ([LangChain Docs][3])\n",
        "* **Distributed tracing**: If your app is microservices or spans multiple services, LangSmith can link spans across them (as long as you pass headers / context). ([LangChain Docs][3])\n",
        "* You need to ensure all traces get submitted before process exit ‚Äî in serverless environments especially, the process may exit before async traces flush. LangSmith provides utilities / configurations to wait for flush or make callbacks synchronous. ([LangChain Docs][3])\n",
        "\n",
        "### 2.2 Evals (Evaluation)\n",
        "\n",
        "* Purpose: You want to **measure quality** of your LLM app systematically ‚Äî not just ‚Äúit seems good,‚Äù but ‚Äúaccuracy is 85%,‚Äù ‚Äúbleu score is X,‚Äù ‚Äúuser rating is 4.2.‚Äù\n",
        "* Core concepts:\n",
        "\n",
        "  * **Dataset**: A set of ‚Äúinput ‚Üí reference output (ground truth)‚Äù pairs (plus optional metadata) to test your app. ([LangChain Docs][4])\n",
        "  * **Example**: A single item in that dataset (inputs dict, outputs dict, metadata) ([LangChain Docs][4])\n",
        "  * **Evaluator**: A function (or mechanism) that scores the model‚Äôs output vs the reference (or judge quality). There are types:\n",
        "\n",
        "    * **Heuristic** (rule-based)\n",
        "    * **Human evaluation** (humans rate outputs)\n",
        "    * **LLM-as-Judge** (you prompt an LLM to act as the grader) ([LangChain Docs][4])\n",
        "  * **Experiment / Run**: Running your model (or app version) over the dataset, collecting outputs, and scoring them. You can compare experiments (versions) over time. ([LangChain Docs][4])\n",
        "  * **Summary evaluators**: Once you have per-example scores, you can aggregate metrics (mean, precision, recall, etc.) ([LangSmith Docs][5])\n",
        "* The SDK (`Client.evaluate`) lets you run evaluations programmatically (sync or async) on datasets you manage in LangSmith. ([LangSmith Docs][5])\n",
        "* Evals help you detect regressions or improvements when you change prompts, models, or system logic.\n",
        "\n",
        "### 2.3 Prompt engineering / iteration / versioning / collaboration\n",
        "\n",
        "* UI + tools to build and test prompts interactively (Playground) ‚Äî try variations, see outputs live. ([LangSmith Docs][1])\n",
        "* Automatic **version control** of prompt templates, so you can track which version produced which outputs. Useful for audits, rollback. ([LangSmith Docs][1])\n",
        "* Collaboration: teammates can comment, recommend, share prompts or prompt variants.\n",
        "* Compare outputs across prompt versions side by side.\n",
        "* Link prompt versions to trace runs or evaluation experiments.\n",
        "\n",
        "### 2.4 Dashboards / Metrics / Alerts / Monitoring\n",
        "\n",
        "* Key metrics to monitor:\n",
        "\n",
        "  * Latency / response time\n",
        "  * Error rates / failures\n",
        "  * Cost (token usage, API cost)\n",
        "  * Quality metrics (scoring)\n",
        "* Build custom dashboards in LangSmith UI for business-critical metrics.\n",
        "* Set alerts (e.g. when error rate > threshold) and get notified.\n",
        "* Drill down into traces from metrics (e.g. see which runs caused high latency) to root cause.\n",
        "* Track quality over time to detect drift or degradation.\n",
        "\n",
        "### 2.5 Data / Versioning / Projects / Metadata\n",
        "\n",
        "* **Datasets**, **Examples**, **Runs**, **Projects** are organized in LangSmith.\n",
        "* Versioning: Datasets and prompt versions are version controlled (you can tag, revert).\n",
        "* Metadata & tags: You can attach tags or metadata (e.g. ‚Äúmodel version = v2‚Äù, ‚Äúuser type = premium‚Äù) to runs or examples, for filtering and analysis.\n",
        "* You can self-host LangSmith (enterprise) so data doesn‚Äôt leave your network. ([LangChain][2])\n",
        "\n",
        "---\n",
        "\n",
        "## 3. How to use LangSmith ‚Äî step by step + code\n",
        "\n",
        "Let me walk you through a typical setup + usage pattern. I'll use Python + LangChain as the example, since that‚Äôs common. But JS/TS is possible too.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.1 Setup / installation / config\n",
        "\n",
        "1. **Get an API key / account**\n",
        "\n",
        "   * Sign up on LangSmith, generate an API key. ([LangChain][6])\n",
        "   * Set environment variable:\n",
        "\n",
        "     ```bash\n",
        "     export LANGSMITH_API_KEY=\"your_key_here\"\n",
        "     ```\n",
        "   * (Optional) set other env vars: `LANGSMITH_TRACING`, `LANGSMITH_ENDPOINT`, etc. ([LangChain Docs][3])\n",
        "\n",
        "2. **Install packages**\n",
        "\n",
        "   * `langchain` or `langchain-core` (depending on version)\n",
        "   * `langsmith` (SDK)\n",
        "   * You‚Äôll need the version compatibility; check docs.\n",
        "\n",
        "3. **Enable tracing in your application**\n",
        "\n",
        "   * For LangChain Python, tracing can be enabled with environment variables or via `traceable` context. ([LangChain Docs][3])\n",
        "   * E.g. wrap your function with `@traceable` so everything inside it is traced.\n",
        "   * Or use `with ls.tracing_context(...)` to create a tracing context block. ([LangChain Docs][3])\n",
        "   * After your chain / agent executes, call `wait_for_all_tracers()` to ensure trace flush. ([LangChain Docs][3])\n",
        "\n",
        "4. **Connect client / project**\n",
        "\n",
        "   * In code, you might do:\n",
        "\n",
        "     ```python\n",
        "     from langsmith import Client\n",
        "     client = Client(api_key=\"‚Ä¶\")  # if not using env var\n",
        "     ```\n",
        "   * The client lets you interact with datasets, runs, feedback, etc. ([LangSmith Docs][7])\n",
        "\n",
        "---\n",
        "\n",
        "### 3.2 Tracing example (LangChain + LangSmith)\n",
        "\n",
        "Here‚Äôs a toy example to show the idea:\n",
        "\n",
        "```python\n",
        "from langsmith import tracing_context\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are assistant. Answer based only on context.\"),\n",
        "    (\"user\", \"Question: {question}\\nContext: {context}\")\n",
        "])\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "output_parser = StrOutputParser()\n",
        "chain = prompt | model | output_parser\n",
        "\n",
        "@traceable(tags=[\"my-app\"], metadata={\"env\": \"dev\"})\n",
        "def run_app(question, context):\n",
        "    result = chain.invoke({\"question\": question, \"context\": context})\n",
        "    return result\n",
        "\n",
        "run_app(\"What is the capital of France?\", \"Ignore this\")\n",
        "\n",
        "# after done, flush traces\n",
        "from langchain_core.tracers.langchain import wait_for_all_tracers\n",
        "wait_for_all_tracers()\n",
        "```\n",
        "\n",
        "That will send traces with tags, inputs, outputs, nested steps. ([LangChain Docs][3])\n",
        "\n",
        "You‚Äôll see in LangSmith UI a trace tree showing the prompt, model invocation, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.3 Evaluation example\n",
        "\n",
        "Suppose you have a dataset of QA (question ‚Üí correct answer). You want to see how your model + prompt performs.\n",
        "\n",
        "```python\n",
        "from langsmith import Client\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langsmith.evaluation import LangChainStringEvaluator\n",
        "import random\n",
        "\n",
        "client = Client()\n",
        "\n",
        "# assume dataset already exists in LangSmith\n",
        "dataset_name = \"my-qa-dataset\"\n",
        "\n",
        "def predict(inputs: dict) -> dict:\n",
        "    question = inputs[\"question\"]\n",
        "    # run your model or chain\n",
        "    answer = my_chain.invoke({\"question\": question})  # adapt to your code\n",
        "    return {\"response\": answer}\n",
        "\n",
        "# define evaluator(s)\n",
        "accuracy = LangChainStringEvaluator(\"string_match\")\n",
        "\n",
        "results = client.evaluate(\n",
        "    predict,\n",
        "    data=dataset_name,\n",
        "    evaluators=[accuracy],\n",
        "    summary_evaluators=[accuracy],\n",
        "    description=\"QA eval run\",\n",
        "    experiment_prefix=\"v1 test\"\n",
        ")\n",
        "\n",
        "print(\"Experiment name:\", results.experiment_name)\n",
        "```\n",
        "\n",
        "Then in UI, you'll see per example scores, aggregated metrics. ([LangSmith Docs][5])\n",
        "\n",
        "You can also run evaluation on a subset or examples directly. ([LangSmith Docs][5])\n",
        "\n",
        "---\n",
        "\n",
        "### 3.4 Using LangSmithLoader to fetch dataset as documents\n",
        "\n",
        "LangSmith also offers a document loader, so you can fetch examples (inputs/outputs) from a dataset and use them as `Document` objects in LangChain. ([LangChain][6])\n",
        "\n",
        "```python\n",
        "from langchain_core.document_loaders import LangSmithLoader\n",
        "\n",
        "loader = LangSmithLoader(\n",
        "    dataset_name=\"my-qa-dataset\",\n",
        "    content_key=\"question\",  # the key in inputs dict to treat as document content\n",
        "    limit=50\n",
        ")\n",
        "docs = loader.load()\n",
        "for doc in docs[:3]:\n",
        "    print(doc.page_content, doc.metadata[\"outputs\"])\n",
        "```\n",
        "\n",
        "This is handy to integrate LangSmith data into your chain workflows. ([LangChain][6])\n",
        "\n",
        "---\n",
        "\n",
        "### 3.5 Workflow in real projects\n",
        "\n",
        "Here‚Äôs how you typically integrate LangSmith in your dev cycle:\n",
        "\n",
        "1. **During development / prototyping**:\n",
        "\n",
        "   * Wrap your chain / agent with tracing to inspect behavior\n",
        "   * Use prompt iteration tools (Playground, prompt versions)\n",
        "   * Manually test edge cases, inspect trace trees\n",
        "\n",
        "2. **Set up evaluation datasets**: curate input/answer pairs, edge cases. Use historical data or manual. ([LangChain Docs][4])\n",
        "\n",
        "3. **Baseline experiments / runs**: run your current code on dataset, get metrics.\n",
        "\n",
        "4. **Iteration & versioning**: change prompt, model, chain logic; run new experiments; compare to baseline. Use UI or code to see which changed.\n",
        "\n",
        "5. **Monitoring in production**: trace real user requests, monitor latency, error rate, quality drift. Use dashboards & alerts.\n",
        "\n",
        "6. **Feedback loop**: flag low-quality runs, add them to dataset as examples (discovered failure cases), refine prompts or logic.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Best practices, pitfalls & architecture tips\n",
        "\n",
        "Here‚Äôs wisdom (from experience + docs) so you don‚Äôt screw up:\n",
        "\n",
        "* **Don‚Äôt trace everything** in production blindly ‚Äî too much data, overhead. Sample or filter traces intelligently (e.g. only slow / failed runs).\n",
        "* **Use metadata / tags heavily** ‚Äî tag by model version, user type, prompt version. Helps filtering / debugging.\n",
        "* **Be careful with flush / process exit** ‚Äî in serverless / ephemeral environments, traces might not get posted. Use synchronous flush or wait APIs.\n",
        "* **Design evaluation datasets well** ‚Äî covering edge cases, distribution of inputs, avoid bias.\n",
        "* **Run multiple repetitions** (due to non-determinism) when evaluating; average metrics, detect variance.\n",
        "* **Version control your prompts and chain logic** ‚Äî tie each run or experiment to a code-prompt version.\n",
        "* **Alert on business metrics, not only low-level** ‚Äî e.g. if answer relevance drops, not just error codes.\n",
        "* **Don‚Äôt over-trust LLM-as-judge blindly** ‚Äî review outputs, calibrate the grader prompt.\n",
        "* **Ensure privacy / compliance** ‚Äî if data is sensitive, consider self-hosting LangSmith, mask user data, or use private deployments. ([LangChain][2])\n",
        "* **Distribute tracing context correctly** when chaining microservices ‚Äî propagate trace headers.\n",
        "* **Limit data volume** ‚Äî logs, traces, especially with rich text, can be huge. Use filters, compression, sampling.\n",
        "* **Automate evaluation / CI** ‚Äî integrate eval runs in your dev pipeline so regressions are caught early.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Advanced topics / edge cases / things interviewers may dig into\n",
        "\n",
        "* **Custom evaluators**: building your own scoring logic (e.g. semantic similarity, domain-specific checks)\n",
        "* **Pairwise evaluators**: comparing two model versions‚Äô outputs (which is better) automatically ([LangChain Docs][4])\n",
        "* **Chaining / nested runs / child runs**: how traces nest, how to manage child runs & context propagation\n",
        "* **Interoperability between non-LangChain parts**: how to instrument parts not inside LangChain (e.g. microservices) to trace with LangSmith\n",
        "* **Self-hosting / on-prem** version: when data cannot leave your network or cloud, how to deploy LangSmith yourself\n",
        "* **Scaling & performance**: how to ensure trace logging doesn‚Äôt become bottleneck (async flush, batching)\n",
        "* **Integration with OpenTelemetry (OTEL)**: LangSmith is OTEL-compliant; you can bring existing tracing tools and integrate. ([LangChain][2])\n",
        "* **Fault injection / adversarial example injection**: using LangSmith to simulate bad inputs, see how the system behaves\n",
        "* **Drift detection over time**: integrate evaluation & trace metrics continuously to detect performance drift\n",
        "* **Security / access control / multi-tenant setups**: how to manage teams, permissions, data isolation\n",
        "* **Trace filtering / anonymization**: removing user PII before storing traces\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Interview Questions + Answers (ALL types: basic, design, deep, trick)\n",
        "\n",
        "Below is a big list of possible questions (and how you‚Äôd answer). Use these to prep. I'll group them by category.\n",
        "\n",
        "### Basic / Conceptual\n",
        "\n",
        "1. **What is LangSmith? How is it different from typical logging or monitoring tools?**\n",
        "   *Answer:* LangSmith is a platform for tracing, observability, evaluation, and prompt management specifically built for LLM applications. Traditional logging monitors errors, events, metrics, but can't capture internal LLM steps, prompt sequences, or non-deterministic behavior. LangSmith supports trace trees, evaluation datasets, prompt versioning, and deeper insight into model-internal behavior.\n",
        "\n",
        "2. **Why do LLM-based apps need specialized observability?**\n",
        "   *Answer:* Because LLMs are non-deterministic and involve multiple hidden internal steps (prompting, tool calls, reasoning chains). Errors aren‚Äôt just ‚Äúexceptions‚Äù ‚Äî there are logic failures, hallucinations, prompt mis-specifications. So you need detailed tracing, evaluation metrics, ability to inspect partial outputs.\n",
        "\n",
        "3. **What are Datasets, Examples, Evaluators in LangSmith?**\n",
        "   *Answer:*\n",
        "\n",
        "   * *Dataset* = a collection of test cases (input ‚Üí reference output + metadata)\n",
        "   * *Example* = one test case\n",
        "   * *Evaluator* = a scoring function (rule-based, human, or LLM-as-judge) to compare model output vs reference or judge quality.\n",
        "\n",
        "4. **What is LLM-as-Judge? Pros and cons?**\n",
        "   *Answer:* It‚Äôs using an LLM (with a grading prompt) to act as the ‚Äúevaluator‚Äù ‚Äî i.e. you feed it the model‚Äôs output and reference or criteria, and it scores.\n",
        "   **Pros**: scalable, flexible, can capture nuanced quality.\n",
        "   **Cons**: grader LLM may be inconsistent, introduces bias, requires careful prompt tuning, might ‚Äúhallucinate‚Äù in grading.\n",
        "\n",
        "5. **What is trace / run / child run / span in LangSmith?**\n",
        "   *Answer:* A *run* or *trace* is the full journey of one request through your LLM system, often containing nested *child runs* (e.g. sub-chains, tool calls). A *span* is a unit of execution within the trace.\n",
        "\n",
        "6. **How to integrate LangSmith with code (LangChain)?**\n",
        "   *Answer:* Use environment variables or wrap your functions with `@traceable` or `tracing_context` so operations inside get traced. Use `wait_for_all_tracers()` to flush before exit. Optionally use the LangSmith client for evaluation, dataset management. ([LangChain Docs][3])\n",
        "\n",
        "7. **What is distributed tracing and how does LangSmith support it?**\n",
        "   *Answer:* In systems with multiple microservices, you want to link traces across service boundaries (propagate trace context). LangSmith supports passing trace headers between services so child runs in different services still appear under same trace tree. ([LangChain Docs][3])\n",
        "\n",
        "### Implementation / Coding\n",
        "\n",
        "8. **Show me how you‚Äôd wrap a chain in LangChain + LangSmith to trace execution.**\n",
        "   *Answer:* (I‚Äôd show code like the example above: `@traceable` wrapper, instantiate chain, call, then wait_for_all_tracers.)\n",
        "\n",
        "9. **How to fetch examples from a LangSmith dataset to use in your LangChain chain?**\n",
        "   *Answer:* Use `LangSmithLoader`, specifying dataset_name or dataset_id and content_key, then `.load()` or `.lazy_load()` to get `Document` objects including metadata. ([LangChain][6])\n",
        "\n",
        "10. **How do you run an evaluation programmatically?**\n",
        "    *Answer:* Use `client.evaluate(...)`, passing your prediction function, dataset name/ids, evaluators, summary evaluators, experiment metadata, etc. Then parse result, compare. ([LangSmith Docs][5])\n",
        "\n",
        "11. **Suppose your model outputs are unpredictable (variance). How do you design your evaluation to account for that?**\n",
        "    *Answer:* Run multiple repetitions, average scores, compute variance. Also look at distribution of scores, not just mean. Use confidence intervals. Possibly use pairwise comparisons.\n",
        "\n",
        "12. **How do you ensure traces are flushed before a serverless function terminates?**\n",
        "    *Answer:* Use synchronous flush (set `LANGCHAIN_CALLBACKS_BACKGROUND = false`), or call `wait_for_all_tracers()` before exit. Use callbacks that block until send completes. ([LangChain Docs][3])\n",
        "\n",
        "13. **How to build a custom evaluator (e.g. semantic similarity) in LangSmith?**\n",
        "    *Answer:* You can write your own evaluator function (Python or TS) that takes inputs, model outputs, the reference, and returns a metric dict. Register or pass it in `client.evaluate`. Or use `LangChainStringEvaluator` with custom config or prompt.\n",
        "\n",
        "### Design / Architecture\n",
        "\n",
        "14. **Suppose your user requests hit 1000/s. How would you instrument LangSmith at scale without performance bottleneck?**\n",
        "    *Answer:* Use asynchronous batching of trace submission, sample only a subset of traces (e.g. high-latency or error), compress logs, filter unimportant runs, use lightweight metadata only. Use background threads or decoupled pipeline to ingest traces.\n",
        "\n",
        "15. **How would you detect concept drift or model degradation over time using LangSmith?**\n",
        "    *Answer:* Keep evaluating on a ‚Äúcheck set‚Äù periodically, compare metrics over versions. Monitor quality metrics in dashboard, set alerts on drops. Use sliding-window evaluation. Add new examples from recent failures, retrain or adjust prompt. Use trend analysis.\n",
        "\n",
        "16. **How would you design a multi-tenant LangSmith setup for an enterprise (teams, roles, data isolation)?**\n",
        "    *Answer:* Use private deployment / self-hosting, partition data per workspace or team, enforce role-based permissions, isolate datasets/runs by project or org, enforce encryption, audit logging.\n",
        "\n",
        "17. **If part of your system is not in Python / LangChain (say Go or Java), can you still trace it in LangSmith?**\n",
        "    *Answer:* Yes, by sending traces via OpenTelemetry or LangSmith API (HTTP) from that part. You need to propagate trace context (headers) across boundaries. LangSmith supports OTEL compliance. ([LangChain][2])\n",
        "\n",
        "### Behavioral / Scenario / Edge\n",
        "\n",
        "18. **You see in production that error rate is stable but quality (relevance) drops. What do you do?**\n",
        "    *Answer:*\n",
        "\n",
        "    * Drill down into evaluation / metrics dashboard, find when quality drop began.\n",
        "    * Inspect traces of low-quality runs, see what prompt or reasoning went wrong.\n",
        "    * Compare prompt versions via prompt playground.\n",
        "    * Add poor examples to evaluation dataset and retrain or refine prompt.\n",
        "    * Rollback to previous prompt/model version if needed.\n",
        "    * Also monitor drift in input distribution (user inputs changed).\n",
        "\n",
        "19. **A prompt version broke half the dataset, but improved performance on some edge cases. How do you decide whether to adopt it?**\n",
        "    *Answer:*\n",
        "\n",
        "    * Use evaluation metrics: see aggregated score and per-split metrics.\n",
        "    * Use versioned prompt comparison side-by-side.\n",
        "    * If edge cases super important, you might adopt hybrid: use old prompt for general, new for edge cases.\n",
        "    * Use fallback or routing logic.\n",
        "    * Possibly split traffic (A/B testing) and monitor in production.\n",
        "\n",
        "20. **What are limitations or risks of using LangSmith?**\n",
        "    *Answer:*\n",
        "\n",
        "    * Overhead / performance cost (if tracing everything)\n",
        "    * Data privacy / PII leakage via traces\n",
        "    * Over-reliance on LLM-as-judge without human review\n",
        "    * Misleading metrics if dataset is biased or unrepresentative\n",
        "    * Trace explosion / storage & cost escalation\n",
        "    * Complex setup for distributed / multi-service systems\n",
        "\n",
        "---\n",
        "[1]: https://docs.smith.langchain.com/?utm_source=chatgpt.com \"Get started with LangSmith | ü¶úÔ∏èüõ†Ô∏è LangSmith - LangChain\"\n",
        "[2]: https://www.langchain.com/langsmith?utm_source=chatgpt.com \"LangSmith - LangChain\"\n",
        "[3]: https://docs.langchain.com/langsmith/trace-with-langchain?utm_source=chatgpt.com \"Trace with LangChain (Python and JS/TS)\"\n",
        "[4]: https://docs.langchain.com/langsmith/evaluation-concepts?utm_source=chatgpt.com \"Evaluation Concepts - Docs by LangChain\"\n",
        "[5]: https://docs.smith.langchain.com/reference/python/client/langsmith.client.Client?utm_source=chatgpt.com \"Client ‚Äî ü¶úÔ∏èüõ†Ô∏è LangSmith documentation\"\n",
        "[6]: https://python.langchain.com/docs/integrations/document_loaders/langsmith/?utm_source=chatgpt.com \"LangSmithLoader | ü¶úÔ∏è LangChain\"\n",
        "[7]: https://docs.smith.langchain.com/reference/python/client?utm_source=chatgpt.com \"client ‚Äî ü¶úÔ∏èüõ†Ô∏è LangSmith documentation\"\n"
      ],
      "metadata": {
        "id": "tST4hzIYLc8l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yaAu3NTKLh3i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}